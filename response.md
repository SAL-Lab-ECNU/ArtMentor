a) respond to reviewer comments

Dear Reviewers,

We extend our heartfelt gratitude for the insightful and constructive feedback. All revisions, highlighted in red text, are meticulously detailed in the Revised Marked-Up Submission (PDF). It is crucial for clarity that our paper focuses on the multimodal perception, recognition, understanding, and reasoning capabilities of Multimodal Large Language Models (MLLMs) (in Section 2.1; Huang & Zhang, 2024, [arXiv](https://arxiv.org/abs/2408.15769)), as encapsulated in our title “to Explore Multimodal Large Language Models Capabilities.” A key discussion centers on the capabilities of machines in human-AI interaction during the evaluation of artworks, and whether MLLMs can operate as independent entities within this human-agent collaborative evaluation process to assist teachers.

Our space (including 4 systems) is designed not only to comprehensively evaluate these four capabilities of MLLMs through the assessment of artworks but also to reduce the time art teachers spend on evaluating artworks with the help of AI—a dual-benefit approach. It is worth mentioning that this design space thinking is not limited to the evaluation of artworks; it can be applied to any other domain that requires interaction with MLLMs and the assessment of their capabilities, offering a versatile paradigm.

Our key aim is not to create an outstanding AI-assisted art evaluation system, but to carefully record data from this human-computer interactive assessment work, thereby evaluating the performance of MLLMs across multiple dimensions. The implementation of a multi-agent framework is designed to enable targeted enhancements to specific agents if any particular capability reveals shortcomings, thus circumventing the need for costly and extensive micro-adjustments to the entire MLLM. We are confident that these revisions meet the required standards for the Conference and this work makes some added value to the existing body of knowledge on the MLLM for artwork assessment. Detailed one-to-one responses are as below:

**To Reviewer 1**:
Firstly, we sincerely appreciate the detailed review and valuable feedback. In response to the reviews on the novelty of our approach, we have conducted a thorough reflection and clarification. Our method differs significantly from previous HCI or assessment of MLLM capabilities in terms of multimodality, iterative upgrades, and explainability:

1. In multimodality, Mina Lee's work primarily involves designing and analyzing a large human-AI collaborative writing dataset called CoAuthor, to explore and evaluate the capabilities of GPT-3 in assisting creative and argumentative writing (Lee et al., 2022; [ACM Digital Library](https://dl.acm.org/doi/abs/10.1145/3491102.3502030)). Mina Lee's work provides an excellent paradigm for evaluating generative AI (Section 2.3.1), but it only involves text generation capabilities and cannot assess the capabilities of MLLMs. Therefore, we have made technical and theoretical improvements to Mina Lee's work.

2. In iterative upgrades, Mina Lee's work, which utilizes GPT-3 without distinguishing between different sections of the writing process, reveals a critical need for tailored capabilities. Specifically, the middle sections of writing often require extensive content expansion and detailed narration, leveraging GPT-3's capabilities to elaborate; the concluding sections from GPT-3's summarization skills. Given that GPT-3's performance varies across these distinct capabilities, Mina Lee's current framework poses challenges for making cost-effective, targeted adjustments. Any modification necessitates a holistic overhaul of all writing capabilities, precluding isolated enhancements to either expansion or summarization capabilities. To address this, we propose a multi-agent framework for the evaluation of artworks, which allows for a more nuanced segmentation of the assessment process. This paradigm enables precise adjustments to specific capabilities of MLLMs, facilitating iterative system upgrades and optimization (Section 2.3).

3. In explainability, as of 2024, the evaluation of MLLMs predominantly relies on three methodologies: human evaluation, GPT-4 evaluation, and metric evaluation (Section 2.1.2; Huang & Zhang, 2024). However, each of these methods exhibits significant limitations. Human evaluation, due to its inherent subjectivity, is often questioned and typically provides only rough, subjective ratings of MLLMs’ capabilities. While GPT-4 evaluation introduces machine automation, it falls short in terms of explanatory power. Metric evaluation, although offering standardized and objective measures such as accuracy, relies excessively on the final outputs generated by MLLMs and neglects processual data, thereby hindering a deep explanation of the evaluation results, such as the specific reasons for low explainability. To address these challenges, we propose an innovative evaluation paradigm—interactive evaluation between MLLMs and art teachers. This paradigm draws inspiration from metrics in the fields of machine learning and natural language processing, aiming to achieve two benefits: firstly, it significantly reduces the time cost for art teachers through multiple rounds of interaction while achieving the most satisfactory evaluation results for the teachers; secondly, by meticulously extracting key indicators from the initial evaluation to the satisfactory evaluation process, we can throughly explore the capabilities of multimodal large models, thereby providing a more comprehensive and in-depth understanding for the evaluation of MLLMs.

To address Reviewer 1’s reviews and for the benefit of all readers, we have added and revised relevant sections (Section 2.1, 2.2, and 2.3).

Secondly, addressing the concern that the description of “contributions” in the abstract and introduction (Section 1) was broad and somewhat generic, we concurred with the reviewer’s perspective. To rectify this, we have further clarified the unique advantages of ArtMentor in the evaluation of MLLMs, comparing it with existing literature to highlight its unique contributions. Moreover, to prevent any misunderstandings among reviewers and readers, we have explicitly stated that our work utilizes the medium of artwork evaluation to explore the capabilities of MLLMs. Our research also aims to tackle the question of whether an entity at the intersection of the arts, education, and AI/MLLMs can serve as an independent assistant in the evaluation of artworks. This exploration holds  significant implications for grasping the extent to which MLLMs can influence human-machine interaction in art education. Additionally, it is crucial to note that the design of our evaluation framework, rooted in the specific task of artwork assessment, is inherently versatile. This methodology is not limited to artwork evaluation alone; it can be readily adapted to other tasks that utilized MLLMs. For instance, the same approach can be applied to the evaluation of MLLMs through tasks such as photography assessment. Indeed, our framework has the potential to be widely used across virtually all multimodal tasks, providing a comprehensive means to evaluate the capabilities of MLLMs in various domains. We have rewritten the abstract and introduction sections to ensure a more specific and clear articulation of the article’s contributions, emphasizing these key points. Furthermore, we have added relevant comparative analysis in the text to highlight the uniqueness and advantages of ArtMentor, particularly in the context of its potential impact on art education and human-machine interaction, as well as its broader applicability to a wide range of multimodal tasks.

Thirdly, we acknowledge that conciseness is key to enhancing the readability and impact of our paper. In response to this review, we have undertaken a thorough review of the manuscript and identified areas where ideas were unnecessarily repeated. Specifically, we have carefully examined the methodology (Section 4) and removed redundant statements, ensuring that each point is made clearly and succinctly without repetition. In instances where similar ideas were presented across different sections, we have consolidated them into a single, coherent explanation, reducing repetition and enhancing clarity. Beyond the methodology (Section 4), we have also reviewed the entire manuscript to ensure that all sections are concise and contribute uniquely to the overall narrative of our work. We believe these revisions have significantly improved the paper’s readability and focus, making it more concise and engaging for the readers.

Fourthly, we thank the reviewer for highlighting the need for clearer definitions of specific terms, such as GPT-4o. We understand that not all readers may be familiar with the intricacies of the models used in our study. To address this, throughout the paper, we have ensured that each term is used in a context that aids understanding. Where necessary, we have added additional sentences, references or paragraphs to explain the significance and application of these terms within the scope of our work.

Fifthly, we are deeply grateful to the reviewer for the insightful feedback on the design principles section of our paper (Section 3, especially in Section 3.1). We recognize the need to more fully justify our chosen evaluation dimensions and to delve deeper into the non-technical and philosophical aspects of our work. In response to the concerns regarding insufficient references and scientific basis, we have conducted a comprehensive literature review and incorporated more relevant research, thereby providing a stronger theoretical foundation for our selection of evaluation dimensions and linking them closely with established theories and practices in art assessment. It is worth pointing out that our research team includes some domain experts of art education who have spearheaded a team of expert art teachers working on a project of national art assessment. In our previous work (not this paper), we integrated Bergson’s philosophy of “creative evolution” into the assessment of elementary school art, deriving two major evaluation dimensions- “creative expression” and “emotional expression”- under the guiding principle of “relational self-expression.” Based on the art curriculum standards issued by the Ministry of Education of China, we adopted a grounded theory approach and selected art teachers, art education researchers, and experienced front-line art teachers from Zhejiang, Jiangsu, Shaanxi, and Sichuan provinces for qualitative research. Previous work aimed to explore, from a life perspective, the evaluation dimensions and specific observation points that should be included in the assessment of elementary school art, with the goal of constructing a “human-beauty” unified art assessment framework under the perspective of “life aesthetics”, composed of the dual dimensions of “creation-emotion.” In light of the need for anonymization and paper length constraints, and considering that the evaluation of artworks is merely one medium among others for assessing MLLMs, we have provided the coding table for the interviews and some representative original interview statements in the revised supplementary materials. Furthermore, we have thoroughly addressed the non-technical and philosophical aspects of our research in Section 3, as extensively discussed in the review, to ensure a comprehensive understanding of our evaluation dimensions and their scientific grounding.

Sixth, regarding the reasons for choosing specific methods over alternatives, and why we opted for a multi-agent system, process-oriented data collection, and modular upgrades, these have been thoroughly elaborated in the first point through a comparison with Mina Lee’s work (Lee et al., 2022).

Seventh, we appreciate the reviewer’s insightful comments regarding the interviews and baseline comparisons. Mina Lee and colleagues have extensively discussed two approaches in HCI research for investigating the generative capabilities of language models (LMs), as detailed in Section 2.2.3 of their paper (Lee et al., 2022): traditional contextual inquiry and emerging interaction logging analysis. Contextual inquiry, involving writers in collaborative writing with LMs followed by interviews, reveals rich insights into how writers interpret LMs’ capabilities in specific contexts. However, the generalizability of these interviews across different writing contexts, non-professional writers, future versions of LMs, or even different configurations of the same model remains uncertain (Lee et al., 2022). On the other hand, interaction logging analysis, while potentially providing fewer rich insights than interviews, can cover relatively diverse contexts across tasks and writers, allowing for a fine-grained analysis of interactions (Lee et al., 2022). Nevertheless, most previous work has considered restricted interaction settings and adapted LMs on specific tasks, limiting their generalizability. Building on this, our work aims to enhance and integrate these two approaches, specifically tailored for multi-modal tasks using MLLMs, distinguishing it from Mina’s work, which is only applicable to essay systems with single-modal LLMs. Recognizing the effectiveness of interviews in capturing writers’ subjective interpretations of MLLMs’ capabilities, we have included interview content in Section 5.2 and corresponded with it in our results analysis (Section 6) to provide a more comprehensive understanding. Additionally, we have adopted the interaction logging analysis approach to supplement the limitations of interviews (baseline) and attempted to validate our model across a broader range of tasks and configurations. We believe that this integrated approach offers a more comprehensive evaluation of MLLMs’ capabilities and provides valuable insights for future research.

**To Reviewer 2**:

First and foremost, we sincerely thank Reviewer 2 for the in-depth review and valuable feedback on the ArtMentor project. Reviewer 2’s insightful reviews have provided us with valuable directions for further refinement and optimization of our work. Reviewer 2’s concerns regarding the effectiveness of art assessment are very pertinent, and we fully agree. We would like to clarify that it is precisely because of our deep concern for the validity of art assessment that we aim to explore the practical utility of MLLMs in evaluating artworks through multiple rounds of interaction between art teachers and MLLMs. In art education, the cultivation of critical thinking skills is of paramount importance, and the assessment of student work should focus more on the underlying narratives and arguments. This principle was thoroughly considered during the initial design of ArtMentor. Our system extends beyond evaluating the final artworks. Drawing on the methodologies of “Thinking Aloud” (Eccles & Arsal, 2017; [DOI](https://doi.org/10.1080/2159676X.2017.1331501)) and “Protocol Analysis” (Ericsson, 2017; [DOI](https://doi.org/10.1002/9781405164535.ch33)), our system also encourages students to create audio recordings that elucidate their creative ideas and critically analyze their own work (Section 5.2.2). The assessments are conducted under the guidance of art teachers and supported by MLLMs, thereby ensuring the integrity of the evaluation process and enhancing the development of students’ critical thinking skills. Furthermore, the assistance of MLLMs alleviates the burden of art assessment on teachers to some extent. We firmly believe that this teacher-led, comprehensive assessment approach can more fully reflect students’ artistic abilities and depth of thought. Additionally, the multi-round interaction process between art teachers and MLLMs provides important evidence for evaluating the capabilities of MLLMs.

Secondly, the ethical dilemmas in the field of AI image generation that Reviewer 2 mentioned are indeed worthy of our concern. We understand that art is a crucial form of human free expression, and any technological intervention must be handled with caution. Throughout the development of ArtMentor, we have always placed ethical considerations at the forefront, ensuring that the application of technology does not infringe upon the subjectivity and creativity of art. It is precisely because of the need for cautious technological intervention that we have undertaken this work. Our future goal is to establish a “teacher-student-machine” triadic interactive art education dialogue system, but we need to investigate whether MLLMs should act as independent entities within this system. One solution to this concern, as mentioned by Reviewer 1, is to interview art teachers about their understanding of MLLMs, which we have discussed in Section 5.2 and subsequently added in the results analysis (Section 6). However, as Mina Lee’s work suggests, in addition to interviews, we need to simulate human-machine interaction scenarios to truly assess the capabilities of MLLMs (Lee et al., 2022). Therefore, we consider using the multi-round human-machine interaction process between art teachers and MLLMs, combined with natural language processing techniques to extract indicators for evaluating the capabilities of MLLMs. We have submitted this work to the top HCI academic conference CHI2025, aiming to call for caution in the intervention of technology in the era of rapid AI development, and looking forward to discussing with scholars worldwide to promote the establishment of a “teacher-student-machine” triadic interactive art education dialogue system, with the hope of making a significant contribution to art education with sound HCI design.

Regarding terms such as “HCI methods” and “HCI spaces,” we will re-examine their usage and provide clearer explanations and more precise definitions in Section 1 and Section 2. Moreover, Reviewer 2’s focus on process-oriented data is very accurate. We believe that the essence of art lies in exploration and expression, not just the final product. This is also the original intention of establishing a “teacher-student-machine” triadic interactive art education dialogue system in the future. In ArtMentor, by recording and analyzing the collaborative assessment process between art teachers and MLLMs, we aim to capture process-oriented data to evaluate the capabilities of MLLMs.

Reviewer 2’s concerns about the potential dehumanization caused by the project have alerted us. We never intend for ArtMentor to be understood as a tool that reduces art and creative expression to algorithms and evaluation metrics. In fact, our original intention is to assist art education through technology, alleviate the burden of art assessment on teachers, and enhance students’ creative abilities and depth of thought. We view art works as the students behind their creation, thus avoiding objectification.

Reviewer 2’s statement that assessment should be understood as an intersubjective experience is very insightful. We completely agree that art assessment should not be a one-way scoring process but a platform for multi-party participation and interactive dialogue. If MLLMs’ capabilities are verified in our work, we are committed to creating an open and interactive assessment environment in the design of the ‘teacher-student-machine’ triadic interactive art education dialogue system. In this environment, students, teachers, and machine (MLLM-driven agent) can participate in the discussion and evaluation of artworks. We believe that this intersubjective interactive experience will help us better understand and appreciate artworks. Additionally, transforming traditional assessment methods into generative assessment also requires the assistance of generative AI (MLLMs). It is worth mentioning that within the dialogue system, students can provide feedback on teachers’ assessments and machine-assisted assessments, raising questions or even challenges. This process deepens their understanding of the assessment process and realizes the transition from “Assessment of Learning (AoL)” to “Assessment as Learning (AaL)” (Schellekens et al., 2021; [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0191491X21001206)).

The establishment of such a dialogue system not only enhances the intersubjective experience but also fosters a more dynamic and reflective learning environment. Lastly, we are grateful for Reviewer 2 suggestion to explore the philosophical implications of our work. The intersection of technology and art raises profound questions about the nature of creativity, the role of the artist, and the future of art education. We will include a philosophical discussion in the concluding section of our paper, reflecting on these issues and inviting further dialogue.

**To Reviewer 3**:
we sincerely thank Reviewer 3 for the in-depth review and valuable feedback on the ArtMentor project. The professional insights provided by Reviewer 3 have offered us valuable directions for improvement.

Firstly, regarding the originality of the work and its connection to adaptive tutoring literature, we fully agree with Reviewer 3’s perspective. We will further emphasize the originality of our work and appropriately cite relevant adaptive tutoring literature to clarify the positioning and contribution of our tool in the field of computer tutoring. We have carefully considered the OATutor mentioned by Reviewer 3, an open-source adaptive tutoring system based on Intelligent Tutoring System (ITS) principles, which aims to promote adaptive learning research and lower the threshold for researchers in replication, extension, and experimentation. 

Secondly, reviewer 3 mentioned that "better describing several aspects of the design and adaptation process". From the perspective of interaction with practitioners in the general architecture of the system, we have referred to Anderson’s article, and our system is closely related to Principle 6: Provide immediate feedback, Principle 7: Adjust the granularity of instruction, and Principle 8: Promote gradual approach to target skills (Anderson, Corbett, Koedinger, & Pelletier, 1995).

Thirdly, regarding the design of the iteration or adaptation process, we appreciate Reviewer 3’s insightful feedback. We have added a detailed explanation of the few-shot learning process in the revised supplementary materials, where the system employs prompt engineering and context learning within a multi-agent framework to dynamically adapt based on teacher feedback, avoiding the high costs of fine-tuning. Teacher-provided scores are incorporated as few-shot examples in subsequent evaluations, enabling continuous improvement. We acknowledge that our previous description lacked clarity, and we now explicitly state that this adaptation process is part of our evaluation design, rather than an ad-hoc observation. The feedback gathered through human-computer interaction (HCI) sessions with teachers supports our results by refining the system’s performance. Over time, this iterative process leads to more consistent scoring between the teachers and the system, demonstrating the potential for MLLMs to serve as independent assistants within the “teacher-student-machine” triadic interactive art education system, effectively aiding teachers in assessment and providing meaningful art feedback.

Fourthly, we are grateful to Reviewer 3 for raising an important question regarding the extensibility of the art tutor tool to different art lessons, styles, and expertise levels—this perspective has indeed provided us with valuable insights. In our educational context, students engage with art from primary school through high school, and we recognize that each stage has its own characteristics and requirements. While our current study focuses on elementary-level art courses, we believe that with additional time and resources, ArtMentor could be validated and adapted for higher levels of art education. Regarding different artistic styles, we have addressed this aspect in Section 5.2.2, where the artworks used in our study were categorized into three distinct groups: narrative illustrations (samples 1-3), Chinese ink paintings (samples 4-7), and artworks adhering to the Egyptian frontal law (samples 8-20). These categories reflect a diverse range of artistic expressions that ArtMentor can assess. However, as noted in our conclusion, the system’s performance in evaluating Chinese ink paintings was less effective, suggesting potential limitations tied to the capabilities of the underlying MLLM. And our system can be extended to other domains, and the styles of the selected artworks are diverse, as reflected in the dataset. Meanwhile, we adopt the concept of design space (Lee et al., 2024; [ACM Digital Library](https://dl.acm.org/doi/full/10.1145/3613904.3642697)), utilizing human-computer interaction datasets and a multi-agent framework to evaluate MLLMs. This paradigm is not only suitable for the assessment of artworks but also widely applicable to multi-modal tasks in other fields.

Fifthly, we appreciate Reviewer 3’s insightful feedback regarding the inspirations behind our system and the concept of dehumanization in AI-driven art evaluation. Our primary inspiration comes from Mina Lee’s work (Lee et al., 2022), which emphasizes constructing large interaction datasets with diverse contexts, subjective interpretations, process orientation, and reusability. Building on these principles, ArtMentor is a novel system designed to evaluate the multimodal perception, recognition, understanding, and reasoning capabilities of MLLMs and serves as the foundation for our envisioned “teacher-student-machine” triadic interactive art education dialogue system. Regarding dehumanization, our approach treats artworks as expressions of students’ abilities, guided by Bergson’s life philosophy, focusing on the creative process rather than objectifying the artworks themselves; we have expanded on this discussion in Section 3.1. While we acknowledge the relevance of the broader discourse on whether AI can create art, our work focuses on the practical application of MLLMs to assist teachers in evaluating student artwork, rather than generating art. The evaluation process relies on analyzing and interpreting artistic elements, where MLLMs can provide meaningful support. We believe this distinction clarifies the scope of our study while leaving room for future exploration of AI’s broader implications in art creation.

Sixthly, we appreciate Reviewer 3’s feedback regarding Figure 2 and have revised it to better illustrate the interaction flow between the multi-agent system, teachers, and students. The updated figure clarifies the role of the HCI dataset and how it supports the agents’ capabilities, as well as the process of iterative feedback. We have also included the student’s role more explicitly, showing how the system’s suggestions indirectly assist students by enhancing teacher feedback. While ArtMentor primarily supports teachers, its mentoring function extends to students through this indirect interaction. We hope this revision addresses the concerns raised.



**To Reviewer 4**:
We sincerely appreciate Reviewer 4’s thorough summary of the review.

In terms of “Framing and Contextualization,” we have delved into key issues across the domains of art, education, and AI/MLLMs, particularly focusing on the capability of MLLMs to assist art teachers in evaluating artworks and their potential to serve as independent entities within the “teacher-student-machine” triadic interactive art education dialogue system in introduction. In response to the review comments, we have further explored the philosophical aspects of human-led art assessment and more rigorously justified the rationale of the assessment dimensions. We have addressed the issue of dehumanization earlier and more comprehensively, incorporating discussions of these fundamental and humanistic issues at the beginning of the paper and in the literature review section, thereby laying a solid foundation for the discussion and contribution of this work.

Regarding “Formative work and design principles,” our main inspirations are drawn from Mina Lee’s works (Lee et al., 2022; Lee et al., 2024). At the CHI 2022 conference, Mina Lee primarily focused on evaluating the writing capabilities of LLMs (GPT-3) through designing human-computer interaction datasets. Later, at CHI 2024, addressing the challenge of the increasingly dispersed research field of writing assistants, which spans various research communities, Mina Lee proposed a design space as an effective method for systematically examining and exploring the diverse landscape of intelligent interactive writing assistants. Mina Lee delved into five key aspects of writing assistants: tasks, users, technology, interaction, and ecosystem. Building on this, our research further adopts an MLLM-driven approach, expanding the scope of tasks to the multimodal domain and breaking down the artistic evaluation capabilities of MLLMs into multiple sub-abilities. Additionally, we employ a multi-agent framework to facilitate subsequent iterative updates of individual sub-abilities. Besides, we have supplemented the interview content in the results and discussion sections to strengthen our arguments.

In “Methodology and Evaluation,” we acknowledge the reviewers’ concerns regarding the limited scope of the evaluation and the need for greater clarity on how teachers engaged with ArtMentor. In response to R1 and R3, we have clarified our analysis by providing a more detailed classification of teacher interactions and incorporated examples from the interviews to better support our findings. While our current focus is on enhancing individual agents through the multi-agent design, we recognize the importance of qualitative insights and plan to prioritize a more comprehensive analysis of teacher feedback in future research. To address the confusion noted by R3, we have included detailed descriptions of the system adaptation mechanism—such as the few-shot learning processes and prompt engineering techniques—in the supplementary materials. We hope these additions provide a clearer understanding of how the system evolves based on teacher feedback and improves over time. 

In the “Discussion” section, we have expanded our discussion to address the key proposition of whether MLLMs can function as independent entities within the “teacher-student-machine” triadic interactive art education dialogue system. Additionally, we have clarified the terms “HCI methods” and “HCI space” by grounding them in relevant literature and providing definitions at the beginning of the section. These revisions aim to more clearly articulate our contributions to HCI and position our work as a distinct advancement in the field. We appreciate the reviewers’ valuable suggestions, which have helped us strengthen the clarity and depth of our discussion.

**b) provide a summary of the changes we made.**

In response to the valuable feedback from all the reviewers, we have undertaken a comprehensive revision of the paper to address their concerns and enhance the overall clarity, rigor, and impact of our work.

In the abstract and introduction, we refined the description of our contributions to ensure specificity and avoid generic statements. We emphasized the unique advantages of ArtMentor, particularly its applicability to artwork evaluation and its potential to enhance human-machine interaction in art education. Additionally, we discussed how our framework can be extended beyond artwork evaluation to other multimodal tasks, such as photography assessment. This revision provides a clearer articulation of our contributions and highlights the versatility of our methodology.

To address the feedback regarding the novelty of our approach, we clarified how our method differs significantly from prior work, particularly Mina Lee’s research on GPT-3 for text generation. We emphasized our focus on multimodality, iterative upgrades, and explainability. Specifically, we explained the limitations of Mina Lee’s text-only framework and highlighted how our multi-agent paradigm enables targeted capability improvements in the context of MLLMs. This clarification was added to Sections 2.1, 2.2, and 2.3 to help readers understand the unique contributions of our approach.

In response to concerns about method selection and justification, we elaborated on the rationale for using a multi-agent system, process-oriented data collection, and modular upgrades. We compared our approach with Mina Lee’s framework and explained how our system addresses the limitations of single-modal, holistic methods. This discussion was integrated into Sections 2.2.3 and 2.3 to provide a clear understanding of our methodological choices.

Regarding the design principles (Section 3), we expanded the theoretical foundation by incorporating relevant literature and justifying our evaluation dimensions. We detailed the philosophical and educational underpinnings of our work, drawing on previous research in art education and assessment. The inclusion of interview coding tables and qualitative data in the supplementary materials further strengthens the scientific grounding of our evaluation framework.
To address the feedback on interviews and baseline comparisons, we integrated both contextual inquiry and interaction logging analysis. We included interview content in Section 5.2 and expanded the results analysis in Section 6 to offer a more comprehensive understanding of our findings. This combined approach captures both subjective insights and diverse interaction data, enhancing the robustness of our evaluation.

For ethical considerations, particularly in the context of AI image generation and potential dehumanization, we emphasized the importance of preserving artistic subjectivity and creativity. We clarified our intent to support, rather than replace, teachers in the evaluation process and highlighted our future goal of developing a “teacher-student-machine” triadic interactive dialogue system. This discussion was incorporated throughout the paper, especially in the conclusion and Sections 5.2 and 6. Moerover, we expanded the philosophical discussion in the conclusion to reflect on the broader implications of our work. This addition invites further dialogue on the intersection of AI, creativity, and art education, ensuring that our research addresses both practical and theoretical concerns.

To enhance the paper’s conciseness and readability, we thoroughly reviewed the manuscript, especially the methodology section (Section 4), by removing redundancies, consolidating similar ideas, and presenting each point succinctly. These improvements clarify the narrative and make the paper more engaging and accessible. Additionally, we ensured terminology clarity by providing precise definitions and explanations for terms like GPT-4o, HCI methods, and HCI spaces in Sections 1 and 2, adding context and references where necessary to support reader understanding.

We also made significant changes to the figures and supplementary materials. In Figure 2, we revised the descriptions of the design process and interaction flow to reduce redundancy and clarify roles. We added detailed descriptions of the system adaptation mechanism, including few-shot learning processes and prompt engineering techniques, in the supplementary materials.

These comprehensive revisions ensure that our paper is clearer, more concise, and better positioned to contribute meaningfully to the fields of art education, HCI, and MLLM research. We sincerely thank all the reviewers for their invaluable feedback, which has greatly improved the quality and depth of our work.
